[MODEL]
# Each run will get it's own unique id and folder in the output directory
model_name = Deep-biggan-bs64-ch128-mxp-n64-trunc0.75
models_dir = models

[DATA]
# Directory it will train the GAN with
train_dir = ~\Programming\Data\frames

# Aspect Ratio is base width : base height
# Upsample layer of 5, base_with 4, base_height 3 is an image of size 128 x 96
base_width = 4
base_height = 3
# Raising the upsample_layer will double the image height and width, and reducing by 1 will halve the image size
upsample_layers = 5

batch_size = 64

# Number of workers for data-loader
workers = 7

[MACHINE]
# Number of GPUs to use. Use 0 for CPU mode.
ngpu = 1

[TRAIN]
num_epochs = 1000

# Applies gradient accumulations, pseudo-batch size will be batch_size * accumulation_iterations
accumulation_iterations = 1

mixed_precision = True

# Saves model and fake images every save_steps
save_steps = 2000

log_steps = 100

# Loss function options: [hinge, mse, bce]
loss_function = hinge

# Learning rates
generator_lr = 0.00005
discriminator_lr = 0.0002

# Beta1 hyper-param for Adam optimizers
beta1 = 0.0
beta2 = 0.999

[MODEL ARCHITECTURE]

# Options: [deep_biggan, biggan, dcgan]
model_type = deep_biggan

# Size of feature maps in generator
ngf = 128

# Size of feature maps in discriminator
ndf = 128

# Whether to use Exponential Moving Averages
generator_ema = True
ema_decay = 0.9999

# Truncation Value - Value of 0 will disable
truncation_value = 0.75

[METRICS]
# Will run the metric every steps_to_eval amount of steps
steps_to_eval = 500

is_metric = False
fid_metric = False
